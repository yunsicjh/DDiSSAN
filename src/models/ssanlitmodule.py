from typing import Any, Dict
import torch
import torch.nn as nn
import torch.nn.functional as F
from lightning import LightningModule
from transformers import BertModel
from torchmetrics import MaxMetric, MeanMetric
from torchmetrics.classification.accuracy import Accuracy
from torchmetrics.classification.f_beta import F1Score
from torchmetrics.classification.confusion_matrix import ConfusionMatrix
from torch_geometric.data import Data, Batch
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

from src.models.components.bilstm_enhancer import BiLSTMEnhancer
from src.models.components.absa_token_aggregator import ABSATokenAggregator
from src.models.components.diffgraph_atten import DifferentialGraphTransformerLayer
from src.models.components.hybrid_graph_attention import HybridGraphTransformerLayer
from src.models.components.diffcross_atten import MultiHeadDifferentialAttention
from src.models.components.semantic_enhancer import (
    MultiLayerCrossAttention,
    GlobalFeatureEnhancer,
    MultiLayerDifferentialCrossAttention,
    PreFusionModule,
    FinalDifferentialAttention,
    ProgressiveDimensionReduction,
)
from src.models.components.cross_modal_attention import (
    GlobalSemanticEnhancer,
    CrossModalInteraction,
)


class FocalLoss(nn.Module):
    """
    Focal Losså®ç°ï¼šè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    è®ºæ–‡ï¼šFocal Loss for Dense Object Detection
    æ”¯æŒç±»åˆ«æƒé‡çš„å¢å¼ºç‰ˆæœ¬
    """

    def __init__(self, alpha=1.0, gamma=2.0, class_weights=None, reduction="mean"):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.class_weights = class_weights
        self.reduction = reduction

    def forward(self, inputs, targets):
        # ç¡®ä¿class_weightsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        weights = self.class_weights
        if weights is not None:
            weights = weights.to(inputs.device)

        # è®¡ç®—åŠ æƒäº¤å‰ç†µæŸå¤±
        ce_loss = F.cross_entropy(inputs, targets, weight=weights, reduction="none")
        # è®¡ç®—pt = exp(-ce_loss)
        pt = torch.exp(-ce_loss)
        # è®¡ç®—focal loss = alpha * (1 - pt)^gamma * ce_loss
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == "mean":
            return focal_loss.mean()
        elif self.reduction == "sum":
            return focal_loss.sum()
        else:
            return focal_loss


class MixedLoss(nn.Module):
    """
    æ··åˆæŸå¤±ç­–ç•¥ï¼šç»“åˆFocal Lossã€åŠ æƒäº¤å‰ç†µå’Œæ ‡ç­¾å¹³æ»‘
    ä¸“é—¨ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    æ”¯æŒé’ˆå¯¹ç‰¹å®šç±»åˆ«çš„é¢å¤–å…³æ³¨ï¼ˆå¦‚neutralç±»ï¼‰
    """

    def __init__(
        self,
        focal_alpha=1.0,
        focal_gamma=2.0,
        class_weights=None,
        label_smoothing=0.15,
        loss_weights=(0.5, 0.3, 0.2),
        neutral_boost=0.0,  # neutralç±»é¢å¤–æƒé‡ç³»æ•°
    ):
        super(MixedLoss, self).__init__()
        self.focal_loss = FocalLoss(focal_alpha, focal_gamma, class_weights)
        self.class_weights = class_weights
        self.label_smoothing = label_smoothing
        self.loss_weights = loss_weights  # (focal, weighted_ce, label_smooth)
        self.neutral_boost = neutral_boost  # neutralç±»å¢å¼ºç³»æ•°

    def _label_smooth_loss(self, logits, targets):
        """æ ‡ç­¾å¹³æ»‘æŸå¤±"""
        confidence = 1.0 - self.label_smoothing
        log_probs = F.log_softmax(logits, dim=-1)
        nll_loss = F.nll_loss(log_probs, targets, reduction="none")
        smooth_loss = -log_probs.mean(dim=-1)
        loss = confidence * nll_loss + self.label_smoothing * smooth_loss
        return loss.mean()

    def forward(self, logits, targets):
        # 1. Focal Loss - å…³æ³¨å›°éš¾æ ·æœ¬
        focal_loss = self.focal_loss(logits, targets)

        # 2. åŠ æƒäº¤å‰ç†µ - å¹³è¡¡ç±»åˆ«
        weights = self.class_weights
        if weights is not None:
            weights = weights.to(logits.device)
        weighted_ce_loss = F.cross_entropy(logits, targets, weight=weights)

        # 3. æ ‡ç­¾å¹³æ»‘æŸå¤± - æ­£åˆ™åŒ–
        smooth_loss = self._label_smooth_loss(logits, targets)

        # 4. Neutralç±»é¢å¤–å…³æ³¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        neutral_penalty = 0.0
        if self.neutral_boost > 0:
            neutral_mask = targets == 1  # neutralç±»ç´¢å¼•ä¸º1
            if neutral_mask.sum() > 0:
                neutral_logits = logits[neutral_mask]
                neutral_targets = targets[neutral_mask]

                # ä½¿ç”¨æ›´å¼ºçš„focal losså‚æ•°å¤„ç†neutralæ ·æœ¬
                neutral_focal = FocalLoss(
                    alpha=self.focal_loss.alpha * 1.5,  # å¢å¼ºalpha
                    gamma=self.focal_loss.gamma + 1.0,  # å¢å¼ºgamma
                    class_weights=self.class_weights,
                )
                neutral_penalty = neutral_focal(neutral_logits, neutral_targets)
                neutral_penalty = neutral_penalty * self.neutral_boost

        # æ··åˆæŸå¤±
        mixed_loss = (
            self.loss_weights[0] * focal_loss
            + self.loss_weights[1] * weighted_ce_loss
            + self.loss_weights[2] * smooth_loss
            + neutral_penalty  # æ·»åŠ neutralç±»é¢å¤–æŸå¤±
        )

        return mixed_loss, {
            "focal_loss": focal_loss.item(),
            "weighted_ce_loss": weighted_ce_loss.item(),
            "smooth_loss": smooth_loss.item(),
            "mixed_loss": mixed_loss.item(),
        }


class SSANLitModule(LightningModule):
    """
    SSANæ¨¡å‹çš„LightningModuleå°è£…
    """

    def __init__(
        self,
        bert_model_name: str = "bert-base-uncased",
        dep_embed_dim: int = 30,
        num_heads: int = 8,
        num_classes: int = 3,
        # å‚æ•°åˆå§‹åŒ–
        init_enable: bool = True,
        init_method: str = "uniform",  
        init_range: float = 0.1,  
        init_exclude_pretrained: bool = True,  
        # BiLSTMå‚æ•°
        bilstm_hidden_dim: int = 128,  # å›ºå®šä¸º128ç»´
        bilstm_num_layers: int = 1,
        bilstm_dropout: float = 0.5,  # å¢å¼ºæ­£åˆ™åŒ–
        # è·¨æ¨¡æ€æ³¨æ„åŠ›å‚æ•°
        cross_modal_layers: int = 1,
        cross_modal_dropout: float = 0.3,  # å¢å¼ºæ­£åˆ™åŒ–
        # åˆ†ç±»å™¨å‚æ•°
        classifier_dropout: float = 0.6,  # å¢å¼ºæ­£åˆ™åŒ–
        use_layer_norm: bool = True,
        # æ–°å¢æ­£åˆ™åŒ–å‚æ•°
        token_aggregator_dropout: float = 0.2,
        bert_dropout: float = 0.1,
        gradient_clip_val: float = 1.0,
        label_smoothing: float = 0.15,
        # ç±»åˆ«ä¸å¹³è¡¡å¤„ç†å‚æ•°
        use_class_weights: bool = True,
        use_focal_loss: bool = True,
        use_mixed_loss: bool = True,  
        focal_alpha: float = 1.0,
        focal_gamma: float = 2.0,
        mixed_loss_weights: tuple = (
            0.5,
            0.3,
            0.2,
        ),  # (focal, weighted_ce, label_smooth)
        neutral_boost: float = 0.0,  # neutralç±»é¢å¤–å…³æ³¨ç³»æ•° (0.0=ä¸å¯ç”¨, 0.3-0.5=æ¨èå€¼)
        dataset_name: str = "restaurants",  # ç”¨äºè‡ªåŠ¨è®¾ç½®ç±»åˆ«æƒé‡
        # å›¾æ³¨æ„åŠ›ç±»å‹é€‰æ‹©
        graph_attention_type: str = "hybrid",  # "differential" æˆ– "hybrid"
        optimizer: torch.optim.Optimizer = None,
        scheduler: torch.optim.lr_scheduler = None,
    ) -> None:
        super().__init__()
        self.save_hyperparameters(logger=False)

        # BERTç¼–ç å™¨ - å¢åŠ dropoutæ­£åˆ™åŒ–
        self.bert = BertModel.from_pretrained(bert_model_name)
        if hasattr(self.hparams, "bert_dropout") and self.hparams.bert_dropout > 0:
            self.bert.config.hidden_dropout_prob = self.hparams.bert_dropout
            self.bert.config.attention_probs_dropout_prob = self.hparams.bert_dropout

        # æ ¸å¿ƒç»„ä»¶ï¼ˆåœ¨configure_modelä¸­åˆå§‹åŒ–ï¼‰
        self.token_aggregator = None
        self.bilstm_enhancer = None
        self.diffgraph_attention = None
        self.hybrid_graph_attention = None
        self.dep_embedding = None

        # æ–°çš„è·¨æ¨¡æ€ç»„ä»¶
        self.global_semantic_enhancer = None
        self.cross_modal_interaction = None
        self.pre_fusion_module = None
        self.classifier = None

        # ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ï¼šè®¾ç½®ç±»åˆ«æƒé‡
        self.class_weights = None
        if use_class_weights:
            
            dataset_weights = {
                "restaurants": torch.tensor(
                    [4.46, 5.65, 1.67]
                ),  
                "laptops": torch.tensor([2.68, 5.02, 2.34]),  
                "tweets": torch.tensor([3.96, 2.01, 2.01]),  
            }
            weights = dataset_weights.get(
                dataset_name.lower(), torch.tensor([1.0, 1.0, 1.0])
            )
            # æƒé‡å½’ä¸€åŒ–
            self.class_weights = weights / weights.sum() * num_classes

        # æŸå¤±å‡½æ•°é…ç½® - æ”¯æŒæ··åˆæŸå¤±ç­–ç•¥
        if use_mixed_loss:
            self.criterion = MixedLoss(
                focal_alpha=focal_alpha,
                focal_gamma=focal_gamma,
                class_weights=self.class_weights,
                label_smoothing=label_smoothing,
                loss_weights=mixed_loss_weights,
                neutral_boost=neutral_boost,  
            )
        elif use_focal_loss:
            self.criterion = FocalLoss(
                alpha=focal_alpha,
                gamma=focal_gamma,
                class_weights=self.class_weights,
                reduction="mean",
            )
            
        else:
            self.criterion = torch.nn.CrossEntropyLoss(
                weight=self.class_weights, label_smoothing=label_smoothing
            )
        self.train_acc = Accuracy(task="multiclass", num_classes=num_classes)
        self.val_acc = Accuracy(task="multiclass", num_classes=num_classes)
        self.test_acc = Accuracy(task="multiclass", num_classes=num_classes)
        self.train_f1 = F1Score(
            task="multiclass", num_classes=num_classes, average="macro"
        )
        self.val_f1 = F1Score(
            task="multiclass", num_classes=num_classes, average="macro"
        )
        self.test_f1 = F1Score(
            task="multiclass", num_classes=num_classes, average="macro"
        )

        self.test_confusion_matrix = ConfusionMatrix(
            task="multiclass", num_classes=num_classes
        )

        # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°
        self.test_f1_per_class = F1Score(
            task="multiclass", num_classes=num_classes, average=None
        )

        # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        self.test_acc_per_class = Accuracy(
            task="multiclass", num_classes=num_classes, average=None
        )

        # å®šä¹‰ç±»åˆ«æ ‡ç­¾æ˜ å°„
        self.class_names = ["negative", "neutral", "positive"]  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´

        # å­˜å‚¨é¢„æµ‹å’Œç›®æ ‡ç”¨äºè¯¦ç»†åˆ†æ
        self.test_predictions = []
        self.test_targets = []
        self.train_loss = MeanMetric()
        self.val_loss = MeanMetric()
        self.test_loss = MeanMetric()
        self.val_acc_best = MaxMetric()
        self.val_f1_best = MaxMetric()
        self.test_f1_best = MaxMetric()
        # å‚æ•°åˆå§‹åŒ–çŠ¶æ€æ ‡è®°ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
        self._params_initialized = False

    def on_after_backward(self) -> None:
        """åœ¨åå‘ä¼ æ’­åè¿›è¡Œæ¢¯åº¦è£å‰ª"""
        if (
            hasattr(self.hparams, "gradient_clip_val")
            and self.hparams.gradient_clip_val > 0
        ):
            torch.nn.utils.clip_grad_norm_(
                self.parameters(), self.hparams.gradient_clip_val
            )

    def configure_model(self) -> None:
        """
        æ ¹æ®æ–°prompt.txté…ç½®å®Œæ•´çš„å¤šæ¨¡æ€æ¨¡å‹æ¶æ„
        """
        if self.token_aggregator is not None:
            return  # å·²é…ç½®

        # è·å–ä¾èµ–è¯æ±‡è¡¨å¤§å°
        if hasattr(self.trainer, "datamodule") and self.trainer.datamodule is not None:
            dep_vocab_size = self.trainer.datamodule.dep_vocab_size
            print(f"Configuring model with dep_vocab_size: {dep_vocab_size}")
        else:
            dep_vocab_size = 100
            print(f"Warning: Using default dep_vocab_size: {dep_vocab_size}")

       
        self.token_aggregator = ABSATokenAggregator(
            hidden_dim=self.bert.config.hidden_size,
            sentence_aggregation="attention",
            aspect_aggregation="mean",
            use_position_encoding=True,
            dropout=self.hparams.token_aggregator_dropout,
        )

       
        self.bilstm_enhancer = BiLSTMEnhancer(
            input_dim=self.bert.config.hidden_size,
            hidden_dim=self.hparams.bilstm_hidden_dim,
            num_layers=self.hparams.bilstm_num_layers,
            dropout=self.hparams.bilstm_dropout,
            bidirectional=True,
        )

        
        self.dep_embedding = nn.Embedding(dep_vocab_size, self.hparams.dep_embed_dim)

       
        if hasattr(self.trainer, "datamodule") and self.trainer.datamodule is not None:
            pos_vocab_size = self.trainer.datamodule.pos_vocab_size
            print(f"Configuring POS embedding with vocab_size: {pos_vocab_size}")
        else:
            pos_vocab_size = 50  
            print(f"Warning: Using default pos_vocab_size: {pos_vocab_size}")

        self.pos_embedding = nn.Embedding(pos_vocab_size, 30)  
        self.position_embedding = nn.Embedding(
            self.hparams.get("max_seq_len", 128), 30
        )  

       
        self.structure_fusion = nn.Linear(768 + 30 + 30, 768)

       
        if self.hparams.graph_attention_type == "hybrid":
            # ä½¿ç”¨æ··åˆå›¾æ³¨æ„åŠ›
            self.hybrid_graph_attention = HybridGraphTransformerLayer(
                in_channels=self.hparams.bilstm_hidden_dim,  
                out_channels=self.hparams.bilstm_hidden_dim, 
                edge_dim=self.hparams.dep_embed_dim,
                heads=self.hparams.num_heads,
                lambda_init=0.8,
                dropout=self.hparams.cross_modal_dropout,  
                concat=False,  
            )
            
        else:
            # ä½¿ç”¨ä¼ ç»Ÿå·®åˆ†å›¾æ³¨æ„åŠ›
            self.diffgraph_attention = DifferentialGraphTransformerLayer(
                in_channels=self.hparams.bilstm_hidden_dim,  
                out_channels=self.hparams.bilstm_hidden_dim,  
                edge_dim=self.hparams.dep_embed_dim,
                heads=self.hparams.num_heads,
                lambda_init=0.8,
                dropout=self.hparams.cross_modal_dropout,  
                concat=False,  
            )
           

       
        self.semantic_cross_attention = MultiLayerCrossAttention(
            d_model=self.hparams.bilstm_hidden_dim,  
            num_heads=self.hparams.num_heads,
            num_layers=3, 
            dropout=self.hparams.cross_modal_dropout,
        )

       
        self.global_feature_enhancer = GlobalFeatureEnhancer(
            d_model=self.hparams.bilstm_hidden_dim, 
            num_heads=self.hparams.num_heads,
            num_layers=3,  
            dropout=self.hparams.cross_modal_dropout,
        )

       
        self.diff_cross_attention_sem = MultiLayerDifferentialCrossAttention(
            d_model=self.hparams.bilstm_hidden_dim,  
            num_heads=self.hparams.num_heads,
            num_layers=3, 
            dropout=self.hparams.cross_modal_dropout,
            lambda_init=0.8,
        )

        self.diff_cross_attention_syn = MultiLayerDifferentialCrossAttention(
            d_model=self.hparams.bilstm_hidden_dim,  
            num_heads=self.hparams.num_heads,
            num_layers=3,  
            dropout=self.hparams.cross_modal_dropout,
            lambda_init=0.8,
        )

        
        self.pre_fusion_module = PreFusionModule(
            d_model=self.hparams.bilstm_hidden_dim  
        )

      
        fusion_dim = self.hparams.bilstm_hidden_dim * 2  
        self.final_diff_attention = FinalDifferentialAttention(
            d_model=fusion_dim,
            num_heads=self.hparams.num_heads,
            num_layers=3, 
            dropout=self.hparams.cross_modal_dropout,
            lambda_init=0.8,
        )

        
        self.progressive_reduction = ProgressiveDimensionReduction(
            input_dim=fusion_dim, 
            hidden_dims=[128, 64], 
            output_dim=32, 
            dropout=self.hparams.classifier_dropout * 0.5,
        )

       
        classifier_layers = [
            nn.Dropout(self.hparams.classifier_dropout * 0.5),
            nn.Linear(32, 16),  
        ]

        if self.hparams.use_layer_norm:
            classifier_layers.append(nn.LayerNorm(16))

        classifier_layers.extend(
            [
                nn.ReLU(),
                nn.Dropout(self.hparams.classifier_dropout * 0.3),
                nn.Linear(16, self.hparams.num_classes),  
            ]
        )

        self.classifier = nn.Sequential(*classifier_layers)

        graph_attention_name = (
            "æ··åˆå›¾æ³¨æ„åŠ›"
            if self.hparams.graph_attention_type == "hybrid"
            else "å·®åˆ†å›¾æ³¨æ„åŠ›"
        )

        

        if getattr(self.hparams, "init_enable", True) and not self._params_initialized:
            init_method = self.hparams.get("init_method", "uniform")
            exclude_pretrained = getattr(self.hparams, "init_exclude_pretrained", True)

            if init_method == "uniform":
                self._initialize_parameters_uniform(
                    init_range=self.hparams.get("init_range", 0.1),
                    exclude_pretrained=exclude_pretrained,
                )
               
            elif init_method == "xavier":
                self._initialize_parameters_xavier(
                    exclude_pretrained=exclude_pretrained
                )
               
            else:
                pass
            self._params_initialized = True

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­æµç¨‹å®ç°SSANæ¨¡å‹
        """
        
        if self.token_aggregator is None:
            self.configure_model()

        batch_size = batch["bert_input_ids"].shape[0]

       
        bert_output = self.bert(
            input_ids=batch["bert_input_ids"],
            attention_mask=batch["bert_attention_mask"],
        ).last_hidden_state  # [batch_size, seq_len, 768]

       
        sentence_word_features, aspect_word_features, aspect_word_mask = (
            self.token_aggregator(
                bert_output=bert_output,
                sentence_mask=batch["sentence_mask"],
                aspect_in_sentence_mask=batch.get("aspect_in_sentence_mask"),
                token_to_subtoken_maps=batch["token_to_subtoken_maps"],
            )
        ) 

       
        batch_size, seq_len = sentence_word_features.shape[:2]

        
        pos_ids = batch["pos_ids"][:, :seq_len]     
        pos_features = self.pos_embedding(pos_ids)  # [batch_size, seq_len, 30]

     
        position_ids = (
            torch.arange(seq_len, device=sentence_word_features.device)
            .unsqueeze(0)
            .expand(batch_size, -1)
        )
        position_features = self.position_embedding(
            position_ids
        )  # [batch_size, seq_len, 30]

        
        enhanced_sentence_features = torch.cat(
            [
                sentence_word_features,  # [batch_size, seq_len, 768]
                pos_features,  # [batch_size, seq_len, 30]
                position_features,  # [batch_size, seq_len, 30]
            ],
            dim=-1,
        )  # [batch_size, seq_len, 828]

       
        sentence_word_features = self.structure_fusion(
            enhanced_sentence_features
        )  # [batch_size, seq_len, 768]

        
        sentence_word_mask = sentence_word_features.abs().sum(dim=-1) > 1e-6
        c_s_bi = self.bilstm_enhancer.enhance_word_features(
            sentence_word_features, sentence_word_mask
        )  # [batch_size, seq_len, 128]

       
        batch_size, seq_len = c_s_bi.shape[:2]

        
        graph_outputs = []

        for i in range(batch_size):
            # å½“å‰æ ·æœ¬çš„ç‰¹å¾å’Œå›¾ç»“æ„
            node_feats = c_s_bi[i]  # [seq_len, 128]
            adj_matrix = batch["adj_matrix"][i]  # [seq_len, seq_len]
            rel_matrix = batch["dep_rel_matrix"][i]  # [seq_len, seq_len]

            # æ„å»ºè¾¹ç´¢å¼•
            edge_index = adj_matrix.nonzero().t().contiguous()

            # æ„å»ºè¾¹å±æ€§
            if edge_index.size(1) > 0:
                edge_types = rel_matrix[edge_index[0], edge_index[1]]
                edge_attr = self.dep_embedding(edge_types)
            else:
                edge_attr = None

            # åº”ç”¨å›¾æ³¨æ„åŠ›
            if edge_index.size(1) > 0:
                if self.hybrid_graph_attention is not None:
                    # ä½¿ç”¨æ··åˆå›¾æ³¨æ„åŠ›
                    graph_output = self.hybrid_graph_attention(
                        node_feats, edge_index, edge_attr
                    )
                else:
                    # ä½¿ç”¨å·®åˆ†å›¾æ³¨æ„åŠ›
                    graph_output = self.diffgraph_attention(
                        node_feats, edge_index, edge_attr
                    )
            else:
                # æ— è¾¹çš„æƒ…å†µï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾
                graph_output = node_feats

            graph_outputs.append(graph_output)

       
        H_syn = torch.stack(graph_outputs, dim=0)  # [batch_size, seq_len, 128]

        
        valid_mask = sentence_word_mask.unsqueeze(
            -1
        ).float()  # [batch_size, seq_len, 1]
        G = (c_s_bi * valid_mask).sum(dim=1) / (
            valid_mask.sum(dim=1) + 1e-8
        )  # [batch_size, 128]

       
        H_sem = c_s_bi  # [batch_size, seq_len, 128]
        H_sem_1 = self.semantic_cross_attention(H_sem, G)  # [batch_size, seq_len, 128]

       
        G_out = self.global_feature_enhancer(G, H_sem)  # [batch_size, 128]

        
        H_sem_out = self.diff_cross_attention_sem(
            H_sem_1, H_syn, H_syn
        )  # [batch_size, seq_len, 128]

       
        H_syn_c = self.diff_cross_attention_syn(
            H_syn, H_sem, H_sem
        )  # [batch_size, seq_len, 128]

       
        H_syn_out = self.pre_fusion_module(H_syn_c, G_out)  # [batch_size, seq_len, 128]

        
        H_out = torch.cat([H_sem_out, H_syn_out], dim=-1)  # [batch_size, seq_len, 256]

        
        H_out_f, H_out_residual = self.final_diff_attention(
            H_out
        )  # éƒ½æ˜¯[batch_size, seq_len, 256]

       
        H_reduced = self.progressive_reduction(
            H_out_residual
        )  # [batch_size, seq_len, 32]


        aspect_in_sentence_mask = batch.get("aspect_in_sentence_mask")
        if aspect_in_sentence_mask is not None:
            # ä½¿ç”¨æ–¹é¢è¯maskæå–ç‰¹å¾
            aspect_mask_expanded = aspect_in_sentence_mask.unsqueeze(
                -1
            ).float()  # [batch_size, seq_len, 1]

            # æå–æ–¹é¢è¯ç‰¹å¾å¹¶æ± åŒ–
            aspect_features = (
                H_reduced * aspect_mask_expanded
            )  # [batch_size, seq_len, 32]
            aspect_representation = aspect_features.sum(dim=1) / (
                aspect_mask_expanded.sum(dim=1) + 1e-8
            )  # [batch_size, 32]

        else:
            # å¦‚æœæ²¡æœ‰æ–¹é¢è¯maskï¼Œä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–
            valid_mask_reduced = valid_mask[
                :, : H_reduced.size(1), :
            ]  # ç¡®ä¿ç»´åº¦åŒ¹é… [batch_size, seq_len, 1]
            aspect_representation = (H_reduced * valid_mask_reduced).sum(dim=1) / (
                valid_mask_reduced.sum(dim=1) + 1e-8
            )  # [batch_size, 32]

        
        logits = self.classifier(aspect_representation)

        return {
            "logits": logits,
            "representations": {
                "H_sem_out": H_sem_out,
                "H_syn_out": H_syn_out,
                "H_out": H_out,
                "H_out_f": H_out_f,
                "aspect_features": aspect_representation,
            },
        }

    def _initialize_parameters_uniform(
        self, init_range: float = 0.1, exclude_pretrained: bool = True
    ) -> None:
        """ä½¿ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚

        æ³¨æ„ï¼šé»˜è®¤è·³è¿‡é¢„è®­ç»ƒBERTçš„å‚æ•°ï¼Œä»…å¯¹è‡ªå®šä¹‰å±‚ï¼ˆLinear/Embedding/LSTM/GRUç­‰ï¼‰æ‰§è¡Œåˆå§‹åŒ–ã€‚

        Args:
            init_range: å‡åŒ€åˆ†å¸ƒèŒƒå›´ [-init_range, init_range]
            exclude_pretrained: æ˜¯å¦æ’é™¤é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚BERTï¼‰
        """
        low, high = -init_range, init_range

        def should_skip(name: str, module: nn.Module) -> bool:
            if not exclude_pretrained:
                return False
            # è·³è¿‡BERTåŠå…¶å­æ¨¡å—
            if name.startswith("bert") or isinstance(module, (BertModel,)):
                return True
            return False

        # é’ˆå¯¹æ¨¡å—ç±»å‹çš„æƒé‡åˆå§‹åŒ–
        for module_name, module in self.named_modules():
            if should_skip(module_name, module):
                continue

            # Linear å±‚
            if isinstance(module, nn.Linear):
                if module.weight is not None:
                    nn.init.uniform_(module.weight, low, high)
                if module.bias is not None:
                    nn.init.uniform_(module.bias, low, high)

            # Embedding å±‚
            elif isinstance(module, nn.Embedding):
                if module.weight is not None:
                    nn.init.uniform_(module.weight, low, high)

            # RNN ç±»
            elif isinstance(module, (nn.LSTM, nn.GRU, nn.RNN)):
                for name, param in module.named_parameters(recurse=False):
                    if param is not None:
                        nn.init.uniform_(param, low, high)

            # 

    def _initialize_parameters_xavier(self, exclude_pretrained: bool = True) -> None:
        """ä½¿ç”¨ Xavier Uniform åˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚

        Xavier åˆå§‹åŒ–æœ‰åŠ©äºä¿æŒå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­çš„ä¿¡å·ç¨³å®šï¼Œé€‚ç”¨äºå¤§å¤šæ•°æ¿€æ´»å‡½æ•°ã€‚   

        Args:
            exclude_pretrained: æ˜¯å¦æ’é™¤é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚BERTï¼‰
        """

        def should_skip(name: str, module: nn.Module) -> bool:
            if not exclude_pretrained:
                return False
            if name.startswith("bert") or isinstance(module, (BertModel,)):
                return True
            return False

        # é’ˆå¯¹æ¨¡å—ç±»å‹çš„æƒé‡åˆå§‹åŒ–
        for module_name, module in self.named_modules():
            if should_skip(module_name, module):
                continue

            # Linear å±‚ - Xavier Uniform
            if isinstance(module, nn.Linear):
                if module.weight is not None:
                    nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)

            # Embedding å±‚ - Normal Xavier
            elif isinstance(module, nn.Embedding):
                if module.weight is not None:
                    nn.init.xavier_normal_(module.weight)

            # LSTM å±‚ - 
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters(recurse=False):
                    if "weight_ih" in name: 
                        nn.init.xavier_uniform_(param)
                    elif "weight_hh" in name:  
                        
                        nn.init.xavier_uniform_(param)
                    elif "bias" in name:  # åç½®åˆå§‹åŒ–ä¸º0
                        nn.init.constant_(param, 0.0)

            # GRU å’Œ RNN
            elif isinstance(module, (nn.GRU, nn.RNN)):
                for name, param in module.named_parameters(recurse=False):
                    if "weight" in name:
                        # æ‰€æœ‰æƒé‡éƒ½ç”¨ Xavier Uniformï¼ˆMPS å…¼å®¹ï¼‰
                        nn.init.xavier_uniform_(param)
                    elif "bias" in name:
                        nn.init.constant_(param, 0.0)

    def _initialize_parameters_kaiming(self, exclude_pretrained: bool = True) -> None:
        """ä½¿ç”¨ Kaiming (He) Normal åˆå§‹åŒ–ï¼Œé’ˆå¯¹ ReLU æ¿€æ´»å‡½æ•°ä¼˜åŒ–ã€‚

        Args:
            exclude_pretrained: æ˜¯å¦æ’é™¤é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚BERTï¼‰
        """

        def should_skip(name: str, module: nn.Module) -> bool:
            if not exclude_pretrained:
                return False
            if name.startswith("bert") or isinstance(module, (BertModel,)):
                return True
            return False

        for module_name, module in self.named_modules():
            if should_skip(module_name, module):
                continue

            # Linear å±‚
            if isinstance(module, nn.Linear):
                if module.weight is not None:
                    nn.init.kaiming_normal_(
                        module.weight, mode="fan_in", nonlinearity="relu"
                    )
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)

            # Embedding å±‚
            elif isinstance(module, nn.Embedding):
                if module.weight is not None:
                    nn.init.normal_(
                        module.weight, mean=0, std=1.0 / (module.weight.size(1) ** 0.5)
                    )

            # LSTM å±‚ 
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters(recurse=False):
                    if "weight_ih" in name:
                        nn.init.kaiming_normal_(
                            param, mode="fan_in", nonlinearity="relu"
                        )
                    elif "weight_hh" in name:
                        # æ”¹ç”¨ Xavier Uniform ä»¥ç¡®ä¿ MPS å…¼å®¹æ€§
                        nn.init.xavier_uniform_(param)
                    elif "bias" in name:
                        nn.init.constant_(param, 0.0)

            # GRU å’Œ RNN
            elif isinstance(module, (nn.GRU, nn.RNN)):
                for name, param in module.named_parameters(recurse=False):
                    if "weight_ih" in name:
                        nn.init.kaiming_normal_(
                            param, mode="fan_in", nonlinearity="relu"
                        )
                    elif "weight_hh" in name:
                        # æ”¹ç”¨ Xavier Uniform ä»¥ç¡®ä¿ MPS å…¼å®¹æ€§
                        nn.init.xavier_uniform_(param)
                    elif "bias" in name:
                        nn.init.constant_(param, 0.0)

    def model_step(
        self, batch: Dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """æ¨¡å‹æ­¥éª¤ï¼šæ”¯æŒæ··åˆæŸå¤±ç­–ç•¥ã€Focal Losså’Œç±»åˆ«åŠ æƒ"""
        output = self.forward(batch)
        logits = output["logits"]
        targets = batch["polarity"]


        if isinstance(self.criterion, MixedLoss):

            loss_result = self.criterion(logits, targets)
            if isinstance(loss_result, tuple):
                loss, loss_details = loss_result

                self.loss_details = loss_details
            else:
                loss = loss_result
        elif isinstance(self.criterion, FocalLoss):

            loss = self.criterion(logits, targets)
        elif self.class_weights is not None and not isinstance(
            self.criterion, (FocalLoss, MixedLoss)
        ):

            criterion = nn.CrossEntropyLoss(
                weight=self.class_weights.to(logits.device),
                label_smoothing=self.hparams.label_smoothing,
            )
            loss = criterion(logits, targets)
        else:
       
            loss = self.criterion(logits, targets)

        preds = torch.argmax(logits, dim=1)
        return loss, preds, targets

    def training_step(
        self, batch: Dict[str, torch.Tensor], batch_idx: int
    ) -> torch.Tensor:
     
        loss, preds, targets = self.model_step(batch)
        batch_size = targets.size(0)

        # æ›´æ–°æŒ‡æ ‡
        self.train_loss(loss)
        self.train_acc(preds, targets)
        self.train_f1(preds, targets)

        # è®°å½•æŒ‡æ ‡
        self.log(
            "train/loss",
            self.train_loss,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )
        self.log(
            "train/acc",
            self.train_acc,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )
        self.log(
            "train/f1",
            self.train_f1,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )

        return loss

    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> None:
        """ç®€åŒ–çš„éªŒè¯æ­¥éª¤"""
        loss, preds, targets = self.model_step(batch)
        batch_size = targets.size(0)

        # æ›´æ–°æŒ‡æ ‡
        self.val_loss(loss)
        self.val_acc(preds, targets)
        self.val_f1(preds, targets)

        # è®°å½•æŒ‡æ ‡
        self.log(
            "val/loss",
            self.val_loss,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )
        self.log(
            "val/acc",
            self.val_acc,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )
        self.log(
            "val/f1",
            self.val_f1,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )

    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> None:
        """æµ‹è¯•æ­¥éª¤"""
        # 1. å‰å‘ä¼ æ’­
        output = self.forward(batch)
        logits = output["logits"]
        targets = batch["polarity"]

        # 2. è®¡ç®—æŸå¤±
        if isinstance(self.criterion, MixedLoss):
            loss_result = self.criterion(logits, targets)
            if isinstance(loss_result, tuple):
                loss, _ = loss_result
            else:
                loss = loss_result
        else:
            loss = self.criterion(logits, targets)

        # 3. è·å–é¢„æµ‹ç»“æœ
        preds = torch.argmax(logits, dim=-1)
        batch_size = targets.size(0)

        # 4. æ›´æ–°æµ‹è¯•æŒ‡æ ‡
        self.test_loss(loss)
        self.test_acc(preds, targets)
        self.test_f1(preds, targets)
        self.test_confusion_matrix(preds, targets)
        self.test_f1_per_class(preds, targets)
        self.test_acc_per_class(preds, targets)

        # 5. å­˜å‚¨é¢„æµ‹ç»“æœç”¨äºè¯¦ç»†åˆ†æ
        self.test_predictions.extend(preds.cpu().numpy().tolist())
        self.test_targets.extend(targets.cpu().numpy().tolist())

        # 6. å®æ—¶æ˜¾ç¤ºåˆ†æï¼ˆå¯¹äºæ¯ä¸ªbatchï¼‰
        self._show_batch_analysis(preds.cpu().numpy(), targets.cpu().numpy(), batch_idx)

        # 7. è®°å½•æŒ‡æ ‡
        self.log(
            "test/loss",
            self.test_loss,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )
        self.log(
            "test/acc",
            self.test_acc,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )
        self.log(
            "test/f1",
            self.test_f1,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            batch_size=batch_size,
        )

    def on_validation_epoch_end(self) -> None:
        """éªŒè¯å‘¨æœŸç»“æŸæ—¶çš„å¤„ç†"""
        acc = self.val_acc.compute()
        f1 = self.val_f1.compute()

        self.val_acc_best(acc)
        self.val_f1_best(f1)

        self.log(
            "val/acc_best", self.val_acc_best.compute(), sync_dist=True, prog_bar=True
        )
        self.log(
            "val/f1_best", self.val_f1_best.compute(), sync_dist=True, prog_bar=True
        )

    def _save_confusion_matrix(self, confusion_matrix: np.ndarray):
        """
        ä¿å­˜æ··æ·†çŸ©é˜µæ•°æ®å’Œå¯è§†åŒ–å›¾è¡¨,å¹¶è®°å½•åˆ°wandb

        Args:
            confusion_matrix: æ··æ·†çŸ©é˜µæ•°ç»„ shape: (num_classes, num_classes)
        """
        # è·å–æ•°æ®é›†åç§°
        dataset_name = self.hparams.get("dataset_name", "unknown").lower()

        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = Path("logs/confusion_matrices") / dataset_name
        save_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ—¶é—´æˆ³
        from datetime import datetime

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. ä¿å­˜åŸå§‹æ··æ·†çŸ©é˜µæ•°æ® (numpyæ ¼å¼)
        np_path = save_dir / f"confusion_matrix_{dataset_name}_{timestamp}.npy"
        np.save(np_path, confusion_matrix)
        print(f"\nğŸ’¾ æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {np_path}")

        # 2. ä¿å­˜ä¸ºJSONæ ¼å¼(ä¾¿äºæŸ¥çœ‹)
        cm_dict = {
            "dataset": dataset_name,
            "confusion_matrix": confusion_matrix.tolist(),
            "class_names": self.class_names,
            "total_samples": int(confusion_matrix.sum()),
            "correct_samples": int(np.diag(confusion_matrix).sum()),
            "accuracy": float(np.diag(confusion_matrix).sum() / confusion_matrix.sum()),
        }

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        per_class_metrics = {}
        for i, class_name in enumerate(self.class_names):
            true_total = confusion_matrix[i, :].sum()
            pred_total = confusion_matrix[:, i].sum()
            true_positive = confusion_matrix[i, i]

            precision = true_positive / pred_total if pred_total > 0 else 0
            recall = true_positive / true_total if true_total > 0 else 0
            f1 = (
                2 * precision * recall / (precision + recall)
                if (precision + recall) > 0
                else 0
            )

            per_class_metrics[class_name] = {
                "precision": float(precision),
                "recall": float(recall),
                "f1_score": float(f1),
                "support": int(true_total),
            }

        cm_dict["per_class_metrics"] = per_class_metrics

        json_path = save_dir / f"confusion_matrix_{dataset_name}_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(cm_dict, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ æ··æ·†çŸ©é˜µJSONå·²ä¿å­˜åˆ°: {json_path}")

        # 3. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        img_paths = self._plot_confusion_matrix(
            confusion_matrix, save_dir, timestamp, dataset_name
        )

        # 4. è®°å½•åˆ° WandB
        self._log_to_wandb(confusion_matrix, cm_dict, img_paths, dataset_name)

    def _plot_confusion_matrix(
        self, cm: np.ndarray, save_dir: Path, timestamp: str, dataset_name: str
    ):
        """
        ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾

        Args:
            cm: æ··æ·†çŸ©é˜µ
            save_dir: ä¿å­˜ç›®å½•
            timestamp: æ—¶é—´æˆ³
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            dict: åŒ…å«æ‰€æœ‰å›¾ç‰‡è·¯å¾„çš„å­—å…¸
        """
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        # 1. ç»˜åˆ¶åŸå§‹è®¡æ•°æ··æ·†çŸ©é˜µ
        sns.heatmap(
            cm,
            annot=True,
            fmt=".0f",
            cmap="Blues",
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            ax=axes[0],
            cbar_kws={"label": "Count"},
        )
        axes[0].set_title(
            f"Confusion Matrix - {dataset_name.upper()} (Count)",
            fontsize=14,
            fontweight="bold",
        )
        axes[0].set_ylabel("True Label", fontsize=12)
        axes[0].set_xlabel("Predicted Label", fontsize=12)

        # 2. ç»˜åˆ¶å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ(æŒ‰è¡Œå½’ä¸€åŒ–,æ˜¾ç¤ºæ¯ä¸ªçœŸå®ç±»åˆ«çš„é¢„æµ‹åˆ†å¸ƒ)
        cm_normalized = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
        cm_normalized = np.nan_to_num(cm_normalized)  # å¤„ç†é™¤ä»¥0çš„æƒ…å†µ

        sns.heatmap(
            cm_normalized,
            annot=True,
            fmt=".2%",
            cmap="YlOrRd",
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            ax=axes[1],
            cbar_kws={"label": "Percentage"},
        )
        axes[1].set_title(
            f"Confusion Matrix - {dataset_name.upper()} (Normalized)",
            fontsize=14,
            fontweight="bold",
        )
        axes[1].set_ylabel("True Label", fontsize=12)
        axes[1].set_xlabel("Predicted Label", fontsize=12)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        cm_img_path = save_dir / f"confusion_matrix_{dataset_name}_{timestamp}.png"
        plt.savefig(cm_img_path, dpi=300, bbox_inches="tight")
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå¯è§†åŒ–å·²ä¿å­˜åˆ°: {cm_img_path}")
        plt.close()

        # 3. ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½æ¡å½¢å›¾
        metrics_img_path = self._plot_per_class_metrics(
            cm, save_dir, timestamp, dataset_name
        )

        return {"confusion_matrix": cm_img_path, "per_class_metrics": metrics_img_path}

    def _plot_per_class_metrics(
        self, cm: np.ndarray, save_dir: Path, timestamp: str, dataset_name: str
    ):
        """
        ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„Precision, Recall, F1-Scoreæ¡å½¢å›¾

        Args:
            cm: æ··æ·†çŸ©é˜µ
            save_dir: ä¿å­˜ç›®å½•
            timestamp: æ—¶é—´æˆ³
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            Path: ä¿å­˜çš„å›¾ç‰‡è·¯å¾„
        """
        fig, ax = plt.subplots(figsize=(10, 6))

        metrics_data = []
        for i, class_name in enumerate(self.class_names):
            true_total = cm[i, :].sum()
            pred_total = cm[:, i].sum()
            true_positive = cm[i, i]

            precision = true_positive / pred_total if pred_total > 0 else 0
            recall = true_positive / true_total if true_total > 0 else 0
            f1 = (
                2 * precision * recall / (precision + recall)
                if (precision + recall) > 0
                else 0
            )

            metrics_data.append(
                {
                    "class": class_name,
                    "precision": precision,
                    "recall": recall,
                    "f1_score": f1,
                }
            )

        # å‡†å¤‡æ•°æ®
        classes = [d["class"] for d in metrics_data]
        precisions = [d["precision"] for d in metrics_data]
        recalls = [d["recall"] for d in metrics_data]
        f1_scores = [d["f1_score"] for d in metrics_data]

        # è®¾ç½®æŸ±çŠ¶å›¾ä½ç½®
        x = np.arange(len(classes))
        width = 0.25

        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        ax.bar(x - width, precisions, width, label="Precision", color="#3498db")
        ax.bar(x, recalls, width, label="Recall", color="#2ecc71")
        ax.bar(x + width, f1_scores, width, label="F1-Score", color="#e74c3c")

        # è®¾ç½®å›¾è¡¨
        ax.set_ylabel("Score", fontsize=12)
        ax.set_title(
            f"Per-Class Metrics - {dataset_name.upper()}",
            fontsize=14,
            fontweight="bold",
        )
        ax.set_xticks(x)
        ax.set_xticklabels(classes, rotation=0)
        ax.legend(loc="lower right")
        ax.set_ylim([0, 1.1])
        ax.grid(axis="y", alpha=0.3)

        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (p, r, f) in enumerate(zip(precisions, recalls, f1_scores)):
            ax.text(
                i - width, p + 0.02, f"{p:.2f}", ha="center", va="bottom", fontsize=8
            )
            ax.text(i, r + 0.02, f"{r:.2f}", ha="center", va="bottom", fontsize=8)
            ax.text(
                i + width, f + 0.02, f"{f:.2f}", ha="center", va="bottom", fontsize=8
            )

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        img_path = save_dir / f"per_class_metrics_{dataset_name}_{timestamp}.png"
        plt.savefig(img_path, dpi=300, bbox_inches="tight")
        plt.close()

        return img_path

    def _log_to_wandb(
        self, cm: np.ndarray, cm_dict: dict, img_paths: dict, dataset_name: str
    ):
        """
        å°†æ··æ·†çŸ©é˜µç»“æœè®°å½•åˆ°WandB

        Args:
            cm: æ··æ·†çŸ©é˜µæ•°ç»„
            cm_dict: æ··æ·†çŸ©é˜µå­—å…¸(åŒ…å«å„ç±»æŒ‡æ ‡)
            img_paths: å›¾ç‰‡è·¯å¾„å­—å…¸
            dataset_name: æ•°æ®é›†åç§°
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„logger
        if not hasattr(self, "logger") or self.logger is None:
            print("âš ï¸ æœªæ£€æµ‹åˆ°logger,è·³è¿‡WandBè®°å½•")
            return

        # æ£€æŸ¥æ˜¯å¦æ˜¯WandB logger
        try:
            import wandb

            # å¦‚æœä½¿ç”¨çš„æ˜¯WandbLogger
            if hasattr(self.logger, "experiment"):
                # 1. è®°å½•æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
                wandb_cm = wandb.plot.confusion_matrix(
                    probs=None,
                    y_true=self.test_targets,
                    preds=self.test_predictions,
                    class_names=self.class_names,
                    title=f"Confusion Matrix - {dataset_name.upper()}",
                )
                self.logger.experiment.log(
                    {f"test/{dataset_name}/confusion_matrix_wandb": wandb_cm}
                )

                # 2. è®°å½•æœ¬åœ°ç”Ÿæˆçš„å›¾ç‰‡
                if (
                    img_paths.get("confusion_matrix")
                    and img_paths["confusion_matrix"].exists()
                ):
                    self.logger.experiment.log(
                        {
                            f"test/{dataset_name}/confusion_matrix_image": wandb.Image(
                                str(img_paths["confusion_matrix"]),
                                caption=f"Confusion Matrix - {dataset_name.upper()}",
                            )
                        }
                    )

                if (
                    img_paths.get("per_class_metrics")
                    and img_paths["per_class_metrics"].exists()
                ):
                    self.logger.experiment.log(
                        {
                            f"test/{dataset_name}/per_class_metrics": wandb.Image(
                                str(img_paths["per_class_metrics"]),
                                caption=f"Per-Class Metrics - {dataset_name.upper()}",
                            )
                        }
                    )

                # 3. è®°å½•å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
                per_class_metrics = cm_dict.get("per_class_metrics", {})
                for class_name, metrics in per_class_metrics.items():
                    self.logger.experiment.log(
                        {
                            f"test/{dataset_name}/{class_name}/precision": metrics[
                                "precision"
                            ],
                            f"test/{dataset_name}/{class_name}/recall": metrics[
                                "recall"
                            ],
                            f"test/{dataset_name}/{class_name}/f1_score": metrics[
                                "f1_score"
                            ],
                            f"test/{dataset_name}/{class_name}/support": metrics[
                                "support"
                            ],
                        }
                    )

                # 4. è®°å½•æ•´ä½“æŒ‡æ ‡
                self.logger.experiment.log(
                    {
                        f"test/{dataset_name}/total_samples": cm_dict["total_samples"],
                        f"test/{dataset_name}/correct_samples": cm_dict[
                            "correct_samples"
                        ],
                        f"test/{dataset_name}/accuracy": cm_dict["accuracy"],
                    }
                )

                # 5. åˆ›å»ºæ±‡æ€»è¡¨æ ¼
                table_data = []
                for class_name in self.class_names:
                    metrics = per_class_metrics.get(class_name, {})
                    table_data.append(
                        [
                            class_name,
                            metrics.get("precision", 0),
                            metrics.get("recall", 0),
                            metrics.get("f1_score", 0),
                            metrics.get("support", 0),
                        ]
                    )

                metrics_table = wandb.Table(
                    columns=["Class", "Precision", "Recall", "F1-Score", "Support"],
                    data=table_data,
                )
                self.logger.experiment.log(
                    {f"test/{dataset_name}/metrics_table": metrics_table}
                )

                print(f"âœ… æ··æ·†çŸ©é˜µå·²è®°å½•åˆ° WandB (æ•°æ®é›†: {dataset_name.upper()})")

        except ImportError:
            print("âš ï¸ wandbæœªå®‰è£…,è·³è¿‡WandBè®°å½•")
        except Exception as e:
            print(f"âš ï¸ WandBè®°å½•å¤±è´¥: {e}")

    def _show_batch_analysis(self, preds, targets, batch_idx):
        """å®æ—¶æ˜¾ç¤ºæ‰¹æ¬¡åˆ†æ - ç®€åŒ–ç‰ˆ"""
        if batch_idx % 50 == 0:  # æ¯50ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡
            correct = (preds == targets).sum()
            total = len(preds)
            accuracy = correct / total if total > 0 else 0
            print(f"Batch {batch_idx}: {correct}/{total} correct ({accuracy:.3f})")

    def configure_optimizers(self) -> Dict[str, Any]:
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = self.hparams.optimizer(params=self.parameters())
        if self.hparams.scheduler is not None:
            scheduler = self.hparams.scheduler(optimizer=optimizer)
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "monitor": "val/loss",
                    "interval": "epoch",
                    "frequency": 1,
                },
            }
        return {"optimizer": optimizer}
