import torch
import torch.nn as nn
from typing import List, Dict, Tuple, Optional


class ABSATokenAggregator(nn.Module):
    """
    ä¸“é—¨ä¸ºABSAä»»åŠ¡è®¾è®¡çš„Tokenèšåˆå™¨

    è§£å†³BERT subtokenåˆ°åŸå§‹wordçº§åˆ«ç‰¹å¾èšåˆé—®é¢˜
    è¾“å…¥: h=[h_cls,h1,â€¦â€¦,hn,h_sep,a1,â€¦â€¦,am,h_sep]
    è¾“å‡º: c_s=[c1,â€¦â€¦,cn], c_a=[c1,â€¦â€¦,cm] (è¯çº§ç‰¹å¾)

    å¤„ç†BERT-SPCæ ¼å¼ï¼š[CLS] sentence [SEP] aspect [SEP]
    """

    def __init__(
        self,
        hidden_dim: int = 768,
        sentence_aggregation: str = "attention",
        aspect_aggregation: str = "mean",
        use_position_encoding: bool = True,
        dropout: float = 0.1,
    ):
        """
        Args:
            hidden_dim: BERTéšè—å±‚ç»´åº¦
            sentence_aggregation: å¥å­è¯èšåˆæ–¹æ³• ("mean", "max", "first", "last", "attention")
            aspect_aggregation: æ–¹é¢è¯èšåˆæ–¹æ³•
            use_position_encoding: æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç 
            dropout: dropoutç‡
        """
        super().__init__()
        self.hidden_dim = hidden_dim
        self.use_position_encoding = use_position_encoding

        # å¥å­è¯èšåˆå™¨
        self.sentence_aggregator = TokenToWordAggregator(
            hidden_dim, sentence_aggregation, True, dropout
        )

        # æ–¹é¢è¯èšåˆå™¨ï¼ˆå¯ä»¥ä½¿ç”¨ä¸åŒçš„ç­–ç•¥ï¼‰
        self.aspect_aggregator = TokenToWordAggregator(
            hidden_dim, aspect_aggregation, True, dropout
        )

        # ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if use_position_encoding:
            self.position_embedding = nn.Embedding(512, hidden_dim)  # æœ€å¤§æ”¯æŒ512ä¸ªè¯

        # ç‰¹å¾å¢å¼ºå±‚
        self.feature_enhancer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
        )

    def forward(
        self,
        bert_output: torch.Tensor,
        sentence_mask: torch.Tensor,
        aspect_in_sentence_mask: torch.Tensor,
        token_to_subtoken_maps: List[Dict[int, List[int]]],
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            bert_output: BERTè¾“å‡º [batch_size, seq_len, hidden_dim]
            sentence_mask: å¥å­åœ¨BERTåºåˆ—ä¸­çš„mask [batch_size, seq_len]
            aspect_in_sentence_mask: æ–¹é¢è¯åœ¨åŸå¥ä¸­çš„mask [batch_size, original_seq_len]
            token_to_subtoken_maps: tokenåˆ°subtokençš„æ˜ å°„

        Returns:
            sentence_word_features: å¥å­è¯çº§ç‰¹å¾ [batch_size, max_words, hidden_dim]
            aspect_word_features: æ–¹é¢è¯è¯çº§ç‰¹å¾ [batch_size, max_aspect_words, hidden_dim]
            word_level_mask: è¯çº§åˆ«çš„æœ‰æ•ˆæ€§mask [batch_size, max_words]
        """
        batch_size = bert_output.shape[0]

        # 1. æå–å¥å­è¯çº§ç‰¹å¾
        sentence_word_features = self._extract_sentence_word_features(
            bert_output, sentence_mask, token_to_subtoken_maps
        )

        # 2. ä»å¥å­è¯çº§ç‰¹å¾ä¸­æå–æ–¹é¢è¯ç‰¹å¾
        aspect_word_features, word_level_mask = self._extract_aspect_word_features(
            sentence_word_features, aspect_in_sentence_mask
        )

        # 3. ç‰¹å¾å¢å¼º
        sentence_word_features = self.feature_enhancer(sentence_word_features)
        aspect_word_features = self.feature_enhancer(aspect_word_features)

        return sentence_word_features, aspect_word_features, word_level_mask

    def _extract_sentence_word_features(
        self,
        bert_output: torch.Tensor,
        sentence_mask: torch.Tensor,
        token_to_subtoken_maps: List[Dict[int, List[int]]],
    ) -> torch.Tensor:
        """æå–å¥å­è¯çº§ç‰¹å¾"""
        batch_size = bert_output.shape[0]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è¯æ•°é‡
        word_counts = [len(token_map) for token_map in token_to_subtoken_maps]
        max_words = max(word_counts) if word_counts else 1

        sentence_word_features = torch.zeros(
            batch_size, max_words, self.hidden_dim, device=bert_output.device
        )

        for b in range(batch_size):
            # è·å–å¥å­éƒ¨åˆ†çš„subtokenç‰¹å¾
            sent_indices = sentence_mask[b].nonzero().squeeze(-1)
            if len(sent_indices) > 0:
                sent_subtokens = bert_output[b, sent_indices]  # [sent_len, hidden_dim]

                # èšåˆä¸ºè¯çº§ç‰¹å¾
                token_map = token_to_subtoken_maps[b]
                for word_idx, subtoken_indices in token_map.items():
                    if word_idx < max_words:
                        # è½¬æ¢ä¸ºç›¸å¯¹ä½ç½®
                        sentence_start = 1  # [CLS]åçš„ä½ç½®
                        relative_indices = []
                        for abs_idx in subtoken_indices:
                            rel_idx = abs_idx - sentence_start
                            if 0 <= rel_idx < sent_subtokens.shape[0]:
                                relative_indices.append(rel_idx)

                        if relative_indices:
                            word_subtokens = sent_subtokens[relative_indices]
                            # ä½¿ç”¨å¥å­èšåˆå™¨
                            aggregated = self._aggregate_with_position(
                                word_subtokens, word_idx, self.sentence_aggregator
                            )
                            sentence_word_features[b, word_idx] = aggregated

        return sentence_word_features

    def _extract_aspect_word_features(
        self,
        sentence_word_features: torch.Tensor,
        aspect_in_sentence_mask: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä»å¥å­è¯çº§ç‰¹å¾ä¸­æå–æ–¹é¢è¯ç‰¹å¾"""
        batch_size, max_sentence_words, hidden_dim = sentence_word_features.shape

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ–¹é¢è¯æ•°é‡
        aspect_word_counts = aspect_in_sentence_mask.sum(dim=1).tolist()
        max_aspect_words = int(max(aspect_word_counts) if aspect_word_counts else 1)

        aspect_word_features = torch.zeros(
            batch_size,
            max_aspect_words,
            hidden_dim,
            device=sentence_word_features.device,
        )
        word_level_mask = torch.zeros(
            batch_size,
            max_aspect_words,
            dtype=torch.bool,
            device=sentence_word_features.device,
        )

        for b in range(batch_size):
            # æ‰¾åˆ°æ–¹é¢è¯åœ¨å¥å­ä¸­çš„ä½ç½®
            aspect_indices = aspect_in_sentence_mask[b].nonzero().squeeze(-1)

            for i, word_idx in enumerate(aspect_indices):
                if i < max_aspect_words and word_idx < sentence_word_features.shape[1]:
                    aspect_word_features[b, i] = sentence_word_features[b, word_idx]
                    word_level_mask[b, i] = True

        return aspect_word_features, word_level_mask

    def _aggregate_with_position(
        self,
        subtoken_features: torch.Tensor,
        word_position: int,
        aggregator: "TokenToWordAggregator",
    ) -> torch.Tensor:
        """å¸¦ä½ç½®ä¿¡æ¯çš„èšåˆ"""
        # åŸºç¡€èšåˆ
        aggregated = aggregator._aggregate_subtoken_features(subtoken_features)

        # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_position_encoding:
            pos_embed = self.position_embedding(
                torch.tensor(word_position, device=subtoken_features.device)
            )
            aggregated = aggregated + pos_embed

        return aggregated

    def get_aspect_representation(
        self,
        aspect_word_features: torch.Tensor,
        word_level_mask: torch.Tensor,
        pooling_method: str = "mean",
    ) -> torch.Tensor:
        """
        è·å–æ–¹é¢è¯çš„æ•´ä½“è¡¨ç¤º

        Args:
            aspect_word_features: æ–¹é¢è¯è¯çº§ç‰¹å¾ [batch_size, max_aspect_words, hidden_dim]
            word_level_mask: è¯çº§åˆ«mask [batch_size, max_aspect_words]
            pooling_method: æ± åŒ–æ–¹æ³• ("mean", "max", "first", "last")

        Returns:
            aspect_representation: æ–¹é¢è¯æ•´ä½“è¡¨ç¤º [batch_size, hidden_dim]
        """
        batch_size = aspect_word_features.shape[0]

        if pooling_method == "mean":
            # è®¡ç®—æœ‰æ•ˆè¯çš„å¹³å‡
            masked_features = (
                aspect_word_features * word_level_mask.unsqueeze(-1).float()
            )
            sum_features = masked_features.sum(dim=1)  # [batch_size, hidden_dim]
            count = word_level_mask.sum(dim=1, keepdim=True).float()  # [batch_size, 1]
            count = torch.clamp(count, min=1)  # é¿å…é™¤é›¶
            aspect_repr = sum_features / count

        elif pooling_method == "max":
            # æœ€å¤§æ± åŒ–ï¼ˆåªè€ƒè™‘æœ‰æ•ˆè¯ï¼‰
            masked_features = aspect_word_features.masked_fill(
                ~word_level_mask.unsqueeze(-1), float("-inf")
            )
            aspect_repr, _ = masked_features.max(dim=1)
            # å¤„ç†å…¨éƒ¨æ— æ•ˆçš„æƒ…å†µ
            all_invalid = ~word_level_mask.any(dim=1)
            aspect_repr[all_invalid] = 0

        elif pooling_method == "first":
            # å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆè¯
            aspect_repr = torch.zeros(
                batch_size, self.hidden_dim, device=aspect_word_features.device
            )
            for b in range(batch_size):
                valid_indices = word_level_mask[b].nonzero().squeeze(-1)
                if len(valid_indices) > 0:
                    aspect_repr[b] = aspect_word_features[b, valid_indices[0]]

        elif pooling_method == "last":
            # å–æœ€åä¸€ä¸ªæœ‰æ•ˆè¯
            aspect_repr = torch.zeros(
                batch_size, self.hidden_dim, device=aspect_word_features.device
            )
            for b in range(batch_size):
                valid_indices = word_level_mask[b].nonzero().squeeze(-1)
                if len(valid_indices) > 0:
                    aspect_repr[b] = aspect_word_features[b, valid_indices[-1]]
        else:
            raise ValueError(f"Unsupported pooling method: {pooling_method}")

        return aspect_repr


class TokenToWordAggregator(nn.Module):
    """
    åŸºç¡€Tokenåˆ°Wordç‰¹å¾èšåˆæ¨¡å—

    å°†BERTçš„subtokençº§ç‰¹å¾èšåˆæˆåŸå§‹è¯çº§ç‰¹å¾
    """

    def __init__(
        self,
        hidden_dim: int = 768,
        aggregation_method: str = "mean",
        use_learnable_weights: bool = False,
        dropout: float = 0.1,
    ):
        """
        Args:
            hidden_dim: BERTéšè—å±‚ç»´åº¦
            aggregation_method: èšåˆæ–¹æ³• ("mean", "max", "first", "last", "attention")
            use_learnable_weights: æ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡è¿›è¡Œèšåˆ
            dropout: dropoutç‡
        """
        super().__init__()
        self.hidden_dim = hidden_dim
        self.aggregation_method = aggregation_method
        self.use_learnable_weights = use_learnable_weights

        # å¦‚æœä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆ
        if aggregation_method == "attention":
            self.attention_layer = nn.Linear(hidden_dim, 1)

        # å¦‚æœä½¿ç”¨å¯å­¦ä¹ æƒé‡
        if use_learnable_weights:
            self.weight_layer = nn.Sequential(nn.Linear(hidden_dim, 1), nn.Sigmoid())

        self.dropout = nn.Dropout(dropout)

    def _aggregate_subtoken_features(
        self, subtoken_features: torch.Tensor
    ) -> torch.Tensor:
        """
        èšåˆå•ä¸ªè¯çš„å¤šä¸ªsubtokenç‰¹å¾

        Args:
            subtoken_features: [num_subtokens, hidden_dim]

        Returns:
            aggregated_feature: [hidden_dim]
        """
        if subtoken_features.shape[0] == 0:
            return torch.zeros(self.hidden_dim, device=subtoken_features.device)

        if self.aggregation_method == "mean":
            # å¹³å‡æ± åŒ–
            aggregated = subtoken_features.mean(dim=0)

        elif self.aggregation_method == "max":
            # æœ€å¤§æ± åŒ–
            aggregated, _ = subtoken_features.max(dim=0)

        elif self.aggregation_method == "first":
            # å–ç¬¬ä¸€ä¸ªsubtoken
            aggregated = subtoken_features[0]

        elif self.aggregation_method == "last":
            # å–æœ€åä¸€ä¸ªsubtoken
            aggregated = subtoken_features[-1]

        elif self.aggregation_method == "attention":
            # æ³¨æ„åŠ›åŠ æƒèšåˆ
            attention_scores = self.attention_layer(
                subtoken_features
            )  # [num_subtokens, 1]
            attention_weights = torch.softmax(
                attention_scores, dim=0
            )  # [num_subtokens, 1]
            aggregated = (subtoken_features * attention_weights).sum(
                dim=0
            )  # [hidden_dim]

        else:
            raise ValueError(
                f"Unsupported aggregation method: {self.aggregation_method}"
            )

        # åº”ç”¨å¯å­¦ä¹ æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_learnable_weights:
            weight = self.weight_layer(aggregated.unsqueeze(0)).squeeze()  # scalar
            aggregated = aggregated * weight

        return self.dropout(aggregated)


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":

    def create_sample_data():
        """åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
        batch_size = 2
        seq_len = 20
        hidden_dim = 768
        original_seq_len = 8

        # æ¨¡æ‹ŸBERTè¾“å‡º
        bert_output = torch.randn(batch_size, seq_len, hidden_dim)

        # æ¨¡æ‹Ÿå¥å­maskï¼š[CLS] The food is great [SEP] food [SEP]
        sentence_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)
        sentence_mask[:, 1:10] = True  # å¥å­éƒ¨åˆ†åœ¨ä½ç½®1-9

        # æ¨¡æ‹Ÿæ–¹é¢è¯åœ¨åŸå¥ä¸­çš„mask
        aspect_in_sentence_mask = torch.zeros(
            batch_size, original_seq_len, dtype=torch.bool
        )
        aspect_in_sentence_mask[0, 2:4] = True  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ–¹é¢è¯åœ¨ä½ç½®2-3
        aspect_in_sentence_mask[1, 1:2] = True  # ç¬¬äºŒä¸ªæ ·æœ¬çš„æ–¹é¢è¯åœ¨ä½ç½®1

        # æ¨¡æ‹Ÿtokenåˆ°subtokenæ˜ å°„
        token_to_subtoken_maps = [
            {0: [1, 2], 1: [3], 2: [4, 5], 3: [6], 4: [7, 8, 9]},  # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼š5ä¸ªè¯
            {0: [1], 1: [2, 3], 2: [4], 3: [5, 6, 7], 4: [8, 9]},  # ç¬¬äºŒä¸ªæ ·æœ¬ï¼š5ä¸ªè¯
        ]

        return {
            "bert_output": bert_output,
            "sentence_mask": sentence_mask,
            "aspect_in_sentence_mask": aspect_in_sentence_mask,
            "token_to_subtoken_maps": token_to_subtoken_maps,
        }

    # æµ‹è¯•èšåˆå™¨
    print("ğŸ” æµ‹è¯•ABSA Tokenèšåˆå™¨...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    sample_data = create_sample_data()

    # åˆ›å»ºèšåˆå™¨
    aggregator = ABSATokenAggregator(
        hidden_dim=768,
        sentence_aggregation="attention",
        aspect_aggregation="mean",
        use_position_encoding=True,
        dropout=0.1,
    )

    # å‰å‘ä¼ æ’­
    sentence_features, aspect_features, aspect_mask = aggregator(**sample_data)

    print(f"âœ… å¥å­è¯çº§ç‰¹å¾å½¢çŠ¶: {sentence_features.shape}")
    print(f"âœ… æ–¹é¢è¯è¯çº§ç‰¹å¾å½¢çŠ¶: {aspect_features.shape}")
    print(f"âœ… æ–¹é¢è¯maskå½¢çŠ¶: {aspect_mask.shape}")

    # æµ‹è¯•æ–¹é¢è¯æ•´ä½“è¡¨ç¤º
    aspect_repr = aggregator.get_aspect_representation(
        aspect_features, aspect_mask, pooling_method="mean"
    )
    print(f"âœ… æ–¹é¢è¯æ•´ä½“è¡¨ç¤ºå½¢çŠ¶: {aspect_repr.shape}")

    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Tokenèšåˆå™¨å·¥ä½œæ­£å¸¸ã€‚")
